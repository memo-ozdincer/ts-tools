Trilliuminfo:

ArbÃ«rishtĞ°Ô¥ÑÑˆÓ™Ğ°bahasa ambonAcÃ¨hKwÃ©yÃ²l Sent LisiØ¹Ø±Ø§Ù‚ÙŠĞ°Ğ´Ñ‹Ğ³Ğ°Ğ±Ğ·ÑĞ°Ğ´Ñ‹Ğ³Ğ°Ğ±Ğ·ÑØªÙˆÙ†Ø³ÙŠ / TÃ»nsÃ®ØªÙˆÙ†Ø³ÙŠTÃ»nsÃ®AfrikaansGegÃ«Ğ°Ğ»Ñ‚Ğ°Ğ¹ Ñ‚Ğ¸Ğ»áŠ áˆ›áˆ­áŠ›PangcaharagonÃ©sÃ†ngliscOboloà¤…à¤‚à¤—à¤¿à¤•à¤¾Ø´Ø§Ù…ÙŠØ§Ù„Ø¹Ø±Ø¨ÙŠØ©ÜÜªÜ¡ÜÜmapudungunØ¬Ø§Ø²Ø§ÙŠØ±ÙŠØ©Ø§Ù„Ø¯Ø§Ø±Ø¬Ø©Ù…ØµØ±Ù‰à¦…à¦¸à¦®à§€à¦¯à¦¼à¦¾American sign languageasturianuAtikamekwĞ°Ğ²Ğ°Ñ€Kotavaà¤…à¤µà¤§à¥€Aymar aruazÉ™rbaycancaØªÛ†Ø±Ú©Ø¬Ù‡Ğ±Ğ°ÑˆÒ¡Ğ¾Ñ€Ñ‚ÑĞ°Basa Baliá¬©á¬²á¬©á¬®á¬¶BoarischBatak TobaBatak TobaØ¬Ù‡Ù„Ø³Ø±ÛŒ Ø¨Ù„ÙˆÚ†ÛŒwawleBikol CentralBajau SamaĞ±ĞµĞ»Ğ°Ñ€ÑƒÑĞºĞ°ÑĞ±ĞµĞ»Ğ°Ñ€ÑƒÑĞºĞ°Ñ (Ñ‚Ğ°Ñ€Ğ°ÑˆĞºĞµĞ²Ñ–Ñ†Ğ°)BetawiĞ±ÑŠĞ»Ğ³Ğ°Ñ€ÑĞºĞ¸à¤¹à¤°à¤¿à¤¯à¤¾à¤£à¤µà¥€Ø±ÙˆÚ† Ú©Ù¾ØªÛŒÙ† Ø¨Ù„ÙˆÚ†ÛŒà¤­à¥‹à¤œà¤ªà¥à¤°à¥€à¤­à¥‹à¤œà¤ªà¥à¤°à¥€BislamaBanjará€•á€¡á€­á€¯á€á€ºá‚á€˜á€¬á‚á€á€¬á‚bamanankanà¦¬à¦¾à¦‚à¦²à¦¾à½–à½¼à½‘à¼‹à½¡à½²à½‚à¦¬à¦¿à¦·à§à¦£à§à¦ªà§à¦°à¦¿à¦¯à¦¼à¦¾ à¦®à¦£à¦¿à¦ªà§à¦°à§€Ø¨Ø®ØªÛŒØ§Ø±ÛŒbrezhonegBrÃ¡huÃ­bosanskiBatak MandailingIriga BicolanoBasa UgiĞ±ÑƒÑ€ÑĞ°Ğ´catalÃ Chavacano de Zamboangağ‘„Œğ‘„‹ğ‘„´ğ‘„Ÿğ‘„³ğ‘„¦é–©æ±èª / MÃ¬ng-dÄ•Ì¤ng-ngá¹³Ì„Ğ½Ğ¾Ñ…Ñ‡Ğ¸Ğ¹Ğ½CebuanoChamoruchinuk wawaá£á³á©TsetsÃªhestÃ¢heseÚ©ÙˆØ±Ø¯ÛŒcorsuCapiceÃ±oè†ä»™èª / PÃ³-sing-gá¹³Ì‚è†ä»™è¯­ï¼ˆç®€ä½“ï¼‰è†ä»™èªï¼ˆç¹é«”ï¼‰NÄ“hiyawÄ“win / á“€á¦áƒá”­ááá£qÄ±rÄ±mtatarcaĞºÑŠÑ‹Ñ€Ñ‹Ğ¼Ñ‚Ğ°Ñ‚Ğ°Ñ€Ğ´Ğ¶Ğ° (ĞšĞ¸Ñ€Ğ¸Ğ»Ğ»)qÄ±rÄ±mtatarca (Latin)tatarÅŸaÄeÅ¡tinakaszÃ«bscziÑĞ»Ğ¾Ğ²Ñ£Ğ½ÑŒÑĞºÑŠ / â°”â°â°‘â°‚â°¡â°â° â°”â°â°ŸÑ‡Ó‘Ğ²Ğ°ÑˆĞ»Ğ°CymraegdanskdagbanliDeutschÃ–sterreichisches DeutschSchweizer HochdeutschDeutsch (Sie-Form)DagaareThuÉ”Å‹jÃ¤Å‹ZazakidolnoserbskiKadazandusunà¤¡à¥‹à¤Ÿà¥‡à¤²à¥€DuÃ¡lÃ¡Ş‹Ş¨ŞˆŞ¬Ş€Ş¨Ş„Ş¦ŞŞ°à½‡à½¼à½„à¼‹à½eÊ‹egbeEfá»‹kemiliÃ n e rumagnÃ²lÎ•Î»Î»Î·Î½Î¹ÎºÎ¬emiliÃ n e rumagnÃ²lEnglishCanadian EnglishBritish EnglishEsperantoespaÃ±olespaÃ±ol (formal)eestieuskaraestremeÃ±uÙØ§Ø±Ø³ÛŒmfantseFulfuldesuomimeÃ¤nkieliNa Vosa VakavitifÃ¸roysktfÉ”Ì€ngbÃ¨franÃ§aisfranÃ§ais cadienarpetanNordfriiskfurlanFryskGaeilgeGaGagauzè´›èªèµ£è¯­ï¼ˆç®€ä½“ï¼‰è´›èªï¼ˆç¹é«”ï¼‰krÃ©yÃ²l GwadloupkriyÃ²l gwiyannenGÃ idhliggalegoĞ½Ğ°Ì„Ğ½Ğ¸Ú¯ÛŒÙ„Ú©ÛŒAvaÃ±e'áº½à¤—à¥‹à¤‚à¤¯à¤šà¥€ à¤•à¥‹à¤‚à¤•à¤£à¥€ / GÃµychi Konknnià¤—à¥‹à¤‚à¤¯à¤šà¥€ à¤•à¥‹à¤‚à¤•à¤£à¥€GÃµychi KonknniBahasa HulontaloğŒ²ğŒ¿ğ„ğŒ¹ğƒğŒºGhanaian Pidginá¼ˆÏÏ‡Î±Î¯Î± á¼‘Î»Î»Î·Î½Î¹Îºá½´Alemannischàª—à«àªœàª°àª¾àª¤à«€wayuunaikifarefaregungbeGaelgHausaå®¢å®¶èª / Hak-kÃ¢-ngÃ®å®¢å®¶è¯­ï¼ˆç®€ä½“ï¼‰å®¢å®¶èªï¼ˆç¹é«”ï¼‰Hak-kÃ¢-ngÃ® (PhaÌk-fa-sá¹³)HawaiÊ»i×¢×‘×¨×™×ªà¤¹à¤¿à¤¨à¥à¤¦à¥€Fiji HindiFiji HindiIlonggoÛÙ†Ø¯Ú©ÙˆhrvatskiHunsrikhornjoserbsceæ¹˜èªKreyÃ²l ayisyenmagyarmagyar (formal)Õ°Õ¡ÕµÕ¥Ö€Õ¥Õ¶Ô±Ö€Õ¥Ö‚Õ´Õ¿Õ¡Õ°Õ¡ÕµÕ¥Ö€Õ§Õ¶interlinguaJaku IbanibibioBahasa IndonesiaInterlingueIgboIgalaê†‡ê‰™IÃ±upiatunáƒá“„á’ƒá‘á‘á‘¦inuktitutIlokanoĞ³Ó€Ğ°Ğ»Ğ³Ó€Ğ°Ğ¹IdoÃ­slenskaĞ¼ĞµĞ´Ğ¶ÑƒÑĞ»Ğ¾Ğ²Ñ˜Ğ°Ğ½ÑĞºÑ‹medÅ¾uslovjanskyitalianoáƒá“„á’ƒá‘á‘á‘¦ / inuktitutæ—¥æœ¬èªPatoisla .lojban.jyskJawaáƒ¥áƒáƒ áƒ—áƒ£áƒšáƒ˜QaraqalpaqshaTaqbaylitKarai-karaiĞ°Ğ´Ñ‹Ğ³ÑĞ±Ğ·ÑĞ°Ğ´Ñ‹Ğ³ÑĞ±Ğ·ÑKabÉ©yÉ›TyapkabuverdianuKongoKumoringÚ©Ú¾ÙˆØ§Ø±GÄ©kÅ©yÅ©KÄ±rmanckiÑ…Ğ°ĞºĞ°Ñá€–á á€¯á€¶á€œá€­á€€á€ºÒ›Ğ°Ğ·Ğ°Ò›ÑˆĞ°Ù‚Ø§Ø²Ø§Ù‚Ø´Ø§ (ØªÙ´ÙˆØªÛ•)Ù‚Ø§Ø²Ø§Ù‚Ø´Ø§ (Ø¬Û‡Ù†Ú¯Ùˆ)Ò›Ğ°Ğ·Ğ°Ò›ÑˆĞ° (ĞºĞ¸Ñ€Ğ¸Ğ»)Ò›Ğ°Ğ·Ğ°Ò›ÑˆĞ° (ÒšĞ°Ğ·Ğ°Ò›ÑÑ‚Ğ°Ğ½)qazaqÅŸa (latÄ±n)qazaqÅŸa (TÃ¼rkÃ¯ya)kalaallisutá—á¶áŸá¶ááŸ’á˜áŸ‚ášà²•à²¨à³à²¨à²¡Yerwa Kanurií•œêµ­ì–´ì¡°ì„ ë§Ğ¿ĞµÑ€ĞµĞ¼ ĞºĞ¾Ğ¼Ğ¸kanuriĞºÑŠĞ°Ñ€Ğ°Ñ‡Ğ°Ğ¹-Ğ¼Ğ°Ğ»ĞºÑŠĞ°Ñ€KrioKinaray-akarjalà¤•à¥‰à¤¶à¥à¤° / Ú©Ù²Ø´ÙØ±Ú©Ù²Ø´ÙØ±à¤•à¥‰à¤¶à¥à¤°Ripoarischá€…á€¾á€®á¤kurdÃ®Ú©ÙˆØ±Ø¯ÛŒ (Ø¹Û•Ø±Û•Ø¨ÛŒ)kurdÃ® (latÃ®nÃ®)ĞºÑŠÑƒĞ¼ÑƒĞºÑŠKÊ‹saalĞºĞ¾Ğ¼Ğ¸kernowekĞºÑ‹Ñ€Ğ³Ñ‹Ğ·Ñ‡Ğ°LatinaLadinoLÃ«tzebuergeschĞ»Ğ°ĞºĞºÑƒĞ»ĞµĞ·Ğ³Ğ¸Lingua Franca NovaLugandaLimburgsLigureLÄ«vÃµ kÄ“Ä¼Ù„Û•Ú©ÛŒLadinlombardlingÃ¡laàº¥àº²àº§SiloziÙ„ÛŠØ±ÛŒ Ø´ÙˆÙ…Ø§Ù„ÛŒlietuviÅ³latgaÄ¼ucilubaMizo Å£awngÙ„Ø¦Ø±ÛŒ Ø¯ÙˆÙ™Ù…ÛŒÙ†ÛŒlatvieÅ¡uæ–‡è¨€LazuriMadhurÃ¢à¤®à¤—à¤¹à¥€à¤®à¥ˆà¤¥à¤¿à¤²à¥€Basa BanyumasanĞ¼Ğ¾ĞºÑˆĞµĞ½ÑŒMalagasyĞ¾Ğ»Ñ‹Ğº Ğ¼Ğ°Ñ€Ğ¸Ğ¹MÄoriMinangkabauĞ¼Ğ°ĞºĞµĞ´Ğ¾Ğ½ÑĞºĞ¸à´®à´²à´¯à´¾à´³à´‚Ğ¼Ğ¾Ğ½Ğ³Ğ¾Ğ»manju gisunmanju gisuná ®á  á ¨á µá¡  á¡¤á¡³á °á¡ á ¨ê¯ƒê¯¤ê¯‡ê¯© ê¯‚ê¯£ê¯Ÿá€˜á€¬á€á€¬á€™á€”á€ºĞ¼Ğ¾Ğ»Ğ´Ğ¾Ğ²ĞµĞ½ÑÑĞºÑmooreà¤®à¤°à¤¾à¤ à¥€MaraĞºÑ‹Ñ€Ñ‹Ğº Ğ¼Ğ°Ñ€Ñ‹Bahasa MelayuØ¨Ù‡Ø§Ø³ Ù…Ù„Ø§ÙŠÙˆMaltiBaso PalembangMirandÃ©sá€™á€¼á€”á€ºá€™á€¬á€˜á€¬á€á€¬ÑÑ€Ğ·ÑĞ½ÑŒÙ…Ø§Ø²ÙØ±ÙˆÙ†ÛŒDorerin NaoeroNÄhuatlé–©å—èª / BÃ¢n-lÃ¢m-gÃºé–©å—èªï¼ˆå‚³çµ±æ¼¢å­—ï¼‰BÃ¢n-lÃ¢m-gÃº (PeÌh-Åe-jÄ«)BÃ¢n-lÃ¢m-gÃº (TÃ¢i-lÃ´)Napulitanonorsk bokmÃ¥lPlattdÃ¼Ã¼tschNedersaksiesà¤¨à¥‡à¤ªà¤¾à¤²à¥€à¤¨à¥‡à¤ªà¤¾à¤² à¤­à¤¾à¤·à¤¾Li Nihaà°•à±Šà°²à°¾à°®à°¿NiuÄ“NederlandsNederlands (informeel)nawdmnorsk nynorská¨£á©¤á©´á¨¾á©®á©¬á©¥á¨¦Ğ½Ğ¾Ğ³Ğ°Ğ¹ÑˆĞ°Novialß’ßßisiNdebele seSewulaNouormandSesotho sa LeboaNupeDinÃ© bizaadChi-ChewarunyankoreOrunyoroNyungaoccitanOjibwemowinlivvinkarjalaOromooà¬“à¬¡à¬¼à¬¿à¬†Ğ¸Ñ€Ğ¾Ğ½à¨ªà©°à¨œà¨¾à¨¬à©€PangasinanKapampanganPapiamentuPicardNaijÃ¡DeitschPlautdietschPÃ¤lzischà¤ªà¤¾à¤²à¤¿Norfuk / PitkernpolskiPiemontÃ¨isÙ¾Ù†Ø¬Ø§Ø¨ÛŒÎ Î¿Î½Ï„Î¹Î±ÎºÎ¬prÅ«siskanÙ¾ÚšØªÙˆportuguÃªsportuguÃªs do BrasilpinayuananMessage documentationRuna SimiRuna shimiRumagnÃ´lTarifitá€›á€á€­á€¯á€„á€ºrumantschromaÅˆi Ähibromani ÄhibikirundiromÃ¢nÄƒtarandÃ­neÑ€ÑƒÑĞºĞ¸Ñ€ÑƒÑÑĞºĞ¸Ğ¹Ñ€ÑƒÑĞ¸Ğ½ÑŒÑĞºÑ‹Ğ¹armÃ£neashtiVlÄƒheÅŸteĞ’Ğ»Ğ°Ñ…ĞµÑÑ‚ĞµVlÄƒheÅŸteĞ¼Ñ‹Ñ…Ğ°Ó€Ğ±Ğ¸ÑˆĞ´Ñ‹Ikinyarwandaã†ã¡ãªãƒ¼ãã¡à¤¸à¤‚à¤¸à¥à¤•à¥ƒà¤¤à¤®à¥ÑĞ°Ñ…Ğ° Ñ‚Ñ‹Ğ»Ğ°á±¥á±Ÿá±±á±›á±Ÿá±²á±¤sardusicilianuScotsØ³Ù†ÚŒÙŠSassaresuÚ©ÙˆØ±Ø¯ÛŒ Ø®ÙˆØ§Ø±Ú¯davvisÃ¡megielladavvisÃ¡megiella (Suoma bealde)davvisÃ¡megiella (Norgga bealde)davvisÃ¡megiella (RuoÅ§a bealde)Cmique ItomKoyraboro SenniSÃ¤ngÃ¶Å¾emaitÄ—Å¡kasrpskohrvatski / ÑÑ€Ğ¿ÑĞºĞ¾Ñ…Ñ€Ğ²Ğ°Ñ‚ÑĞºĞ¸ÑÑ€Ğ¿ÑĞºĞ¾Ñ…Ñ€Ğ²Ğ°Ñ‚ÑĞºĞ¸ (Ñ›Ğ¸Ñ€Ğ¸Ğ»Ğ¸Ñ†Ğ°)srpskohrvatski (latinica)Taclá¸¥itá½á‚ƒá‚‡á€á‚ƒá‚‡á€á‚†á€¸Â tacawittacawità·ƒà·’à¶‚à·„à¶½ĞºÓ£Ğ»Ğ»Ñ‚ ÑĞ°Ì„Ğ¼ÑŒ ĞºÓ£Ğ»Ğ»bidumsÃ¡megiellaslovenÄinaØ³Ø±Ø§Ø¦ÛŒÚ©ÛŒØ³Ø±Ø§Ø¦ÛŒÚ©ÛŒslovenÅ¡ÄinaSchlÃ¤schGagana SamoaÃ¥arjelsaemienanarÃ¢Å¡kielÃ¢nuÃµrttsÃ¤Ã¤Ê¹mÇ©iÃµllchiShonaSoomaaligashqipÑÑ€Ğ¿ÑĞºĞ¸ / srpskiÑÑ€Ğ¿ÑĞºĞ¸ (Ñ›Ğ¸Ñ€Ğ¸Ğ»Ğ¸Ñ†Ğ°)srpski (latinica)Sranantongosardu campidanesuSiSwatiSesothoSeelterskÑĞµĞ±ĞµÑ€Ñ‚Ğ°Ñ‚Ğ°Ñ€SundasvenskaKiswahiliê ê ¤ê Ÿê ê ¤Å›lÅ¯nskiSakizayaà®¤à®®à®¿à®´à¯Tayalà²¤à³à²³à³á¥–á¥­á¥° á¥–á¥¬á¥² á¥‘á¥¨á¥’á¥°à°¤à±†à°²à±à°—à±tetunÑ‚Ğ¾Ò·Ğ¸ĞºÓ£Ñ‚Ğ¾Ò·Ğ¸ĞºÓ£tojikÄ«à¹„à¸—à¸¢á‰µáŒáˆ­áŠ›á‰µáŒáˆ¬TÃ¼rkmenÃ§eTagalogtolÄ±ÅŸiSetswanalea faka-Tongatoki ponaTok PisinTÃ¼rkÃ§eá¹ªuroyoSeediqXitsongaÑ‚Ğ°Ñ‚Ğ°Ñ€Ñ‡Ğ° / tatarÃ§aÑ‚Ğ°Ñ‚Ğ°Ñ€Ñ‡Ğ°tatarÃ§aOrutoorochiTumbukaTwireo tahitiÑ‚Ñ‹Ğ²Ğ° Ğ´Ñ‹Ğ»âµœâ´°âµâ´°âµ£âµ‰âµ–âµœÑƒĞ´Ğ¼ÑƒÑ€Ñ‚Ø¦Û‡ÙŠØºÛ‡Ø±Ú†Û• / UyghurcheØ¦Û‡ÙŠØºÛ‡Ø±Ú†Û•UyghurcheÑƒĞºÑ€Ğ°Ñ—Ğ½ÑÑŒĞºĞ°Ø§Ø±Ø¯ÙˆoÊ»zbekcha / ÑĞ·Ğ±ĞµĞºÑ‡Ğ°TshivendavÃ¨netovepsÃ¤n kelâ€™Tiáº¿ng Viá»‡tWest-VlamsMainfrÃ¤nkischemakhuwaVolapÃ¼kVaÄÄavÃµrowalonwolayttaWinarayFakaÊ»uveaWolofå´è¯­å´è¯­ï¼ˆç®€ä½“ï¼‰å³èªï¼ˆæ­£é«”ï¼‰Ñ…Ğ°Ğ»ÑŒĞ¼Ğ³isiXhosaáƒ›áƒáƒ áƒ’áƒáƒšáƒ£áƒ áƒ˜saisiyat×™×™Ö´×“×™×©YorÃ¹bÃ¡Nháº½áº½gatÃºç²µèªç²µè¯­ï¼ˆç®€ä½“ï¼‰ç²µèªï¼ˆç¹é«”ï¼‰VahcuenghZeÃªuwsâµœâ´°âµâ´°âµ£âµ‰âµ–âµœ âµœâ´°âµâ´°âµ¡â´°âµ¢âµœä¸­æ–‡ä¸­æ–‡ï¼ˆä¸­å›½å¤§é™†ï¼‰ä¸­æ–‡ï¼ˆç®€ä½“ï¼‰ä¸­æ–‡ï¼ˆç¹é«”ï¼‰ä¸­æ–‡ï¼ˆé¦™æ¸¯ï¼‰ä¸­æ–‡ï¼ˆæ¾³é–€ï¼‰ä¸­æ–‡ï¼ˆé©¬æ¥è¥¿äºšï¼‰ä¸­æ–‡ï¼ˆæ–°åŠ å¡ï¼‰ä¸­æ–‡ï¼ˆè‡ºç£ï¼‰isiZulu
Trillium Quickstart
Other languages:
English
Â 
franÃ§ais
Contents
1Overview
2Getting started on Trillium
2.1Logging in
2.2Storage
2.3Software
2.4Tips for loading software
2.5Using commercial software
3Testing and debugging
4Submitting jobs to the scheduler
4.1Trillium specific restrictions
4.1.1Job output must be written to the scratch file system
4.1.2Default scheduler account
4.1.3No job submission from jobs
4.1.4Whole node or whole gpu scheduling
4.1.5Memory requests are ignored
4.2Common options for job script
4.3Submitting jobs on the CPU subcluster
4.3.1Partitions and limits
4.3.2Example: MPI job
4.3.3Example: OpenMP job
4.3.4Example: hybrid MPI/OpenMP job
4.4Submitting jobs for the GPU subcluster
4.4.1Partitions and limits
4.4.2Example: Single-GPU Job
4.4.3Example: Whole-Node (4 GPUs) Job
4.4.4Example: Multi-Node GPU Job
4.4.5Best Practices for GPU Jobs
5Monitoring
5.1Monitoring the queue
5.2Monitoring running and past jobs
6Quick Reference for Common Commands
Overview
Trillium is a large parallel cluster built by Lenovo Canada and hosted by SciNet at the University of Toronto. It consists of three main components:
1. CPU Subcluster
235,008 cores provided by 1224 CPU compute nodes
Each CPU compute node has 192 cores from two 96-core AMD EPYC 9655 CPUs ("Zen 5" a.k.a. "Turin") at 2.6 GHz (base frequency)
Each CPU compute node has 755 GiB / 810 GB of available memory
The nodes are connected by a non-blocking (1:1) 400 Gb/s InfiniBand NDR interconnect
This subcluster is designed for large-scale parallel workloads
2. GPU Subcluster
252 GPUs provided by 63 GPU compute nodes.
Each GPU compute node has 4 NVIDIA H100 (SXM) GPUs with 80 GB of dedicated VRAM
Each GPU compute node also has 96 cores from one 96-core AMD EPYC 9654 CPUs ("Zen 4" a.k.a. "Genoa") at 2.4 GHz (base frequency)
The nodes are connected by a non-blocking (1:1) 800 Gb/s InfiniBand NDR interconnect, i.e. 200 Gb/s per GPU
Has a dedicated login node (trig-login01) with 4 four NVIDIA H100 (SXM) GPUs.
This subcluster is optimized for AI/ML and accelerated science workloads
3. Storage System
Unified 29 PB VAST NVMe storage for all workloads
All flash-based for consistent performance
Accessible as a standard shared parallel file system.
Getting started on Trillium
You need an activeÂ CCDBÂ account from theÂ Digital Research Alliance of Canada. With that, you can then request access to Trillium on theÂ Access SystemsÂ page on theÂ CCDBÂ site. After clicking the "I request access" button, it usually takes about an hour for your account to be actually created and available on Trillium.
Please read this present document carefully. TheÂ Frequently Asked QuestionsÂ is also a useful resource. If at any time you require assistance, or if something is unclear, please do not hesitate toÂ contact us.
Logging in
There are two ways to access Trillium:
Via your browser with Open OnDemand. This is recommended for users who are not familiar with Linux or the command line. Please see ourÂ quickstart guideÂ for more instructions on how to use Open OnDemand.
Terminal access with ssh. Please read the following instructions.
As with all SciNet and Alliance compute systems, access is done viaÂ SSHÂ (secure shell). Furthermore, for Trillium specifically, authentication is only allowed via SSH keys that are uploaded to theÂ CCDB.Â Please refer to this pageÂ on how to generate your SSH key pair, upload, and use SSH Keys.
Trillium runs Rocky Linux 9.6, which is a type of Linux. You will need to be familiar with the Linux shell to work on Trillium. If you are not, it will be worth your time to review theÂ Linux introduction, to attend aÂ Linux Shell course, or to take some of ourÂ Self-paced courses.
You can useÂ SSHÂ by opening a terminal window (e.g.Â Connecting with PuTTYÂ on Windows orÂ Connecting with MobaXTerm), then SSH into the Trillium login nodes with your CCDB credentials.
Use this command to log into one of the login nodes of the CPU subcluster:
$ ssh -i /PATH/TO/SSH_PRIVATE_KEY  MYALLIANCEUSERNAME@trillium.scinet.utoronto.ca

To log into the login node for the GPU cluster, use this command
$ ssh -i /PATH/TO/SSH_PRIVATE_KEY  MYALLIANCEUSERNAME@trillium-gpu.scinet.utoronto.ca

Here,Â /PATH/TO/SSH_PRIVATE_KEYÂ is the path to your private SSH key andÂ MYALLIANCEUSERNAMEÂ is your username on the CCDB.
Note that:
The first time you login, you should make sure you are actually accessing Trillium by checking if theÂ login node ssh host key fingerprintÂ matches.
The Trillium login nodes are where you develop, edit, compile, prepare and submit jobs.
The CPU login nodes and the GPU login node are not part of the compute nodes but they have the same architecture, operating system, and software stack as the CPU and GPU compute nodes, respectively.
You can ssh from one login node to another using their internal hostnamesÂ tri-login01, ..., tri-login06Â andÂ trig-login01Â (the latter is the GPU login node).
If you add the optionÂ -YÂ you enable X11 forwarding, which allows graphical programs on Trillium to open windows on your local computer.
To run on compute nodes, you must submit a batch job.
On the login nodes, you may not:
Run large memory jobs
Run parallel training or highly multi-threaded processes
Run long computations (keep them under a few minutes)
Run resource-intensive tasks like I/O-heavy operations or simulation.
If you cannot log in, be sure to first check theÂ System Status, ensure yourÂ CCDBÂ account is active and that your public key was uploaded (in openssh format) to CCDB, and check that you had requested access on theÂ Access SystemsÂ page.
Storage
Trillium features a unified high-performance storage system based on the VAST platform. It serves the following directories:
home file systemÂ â€“ For personal files and configurations.
scratch file systemÂ â€“ High-speed, temporary personal storage for job data.
project file systemÂ â€“ Shared storage for project teams and collaborations.
For your convenience, the location of the top level of your home and scratch directories on these file systems are available in the environment variablesÂ $HOMEÂ andÂ $SCRATCH, while the variableÂ $PROJECTÂ points at your directory on /project.
You may be part of several projects. In that case,Â $PROJECTÂ points at your last project in alphabetical order (often, that is the one associated with an allocation). But you can find all the top level directories of projects that you have access to inÂ $HOME/links/projects, next to a linkÂ $HOME/links/scratchÂ which points toÂ $SCRATCH. If you do not see the directoryÂ $HOME/linksÂ in your account, you can get it by running the command
$ trisetup

The content of theÂ $HOME/links/projectsÂ will automatically update when you leave or join projects.
OnÂ HPSS, the nearline system to be attached to Trillium, there will also be an environment variable calledÂ $ARCHIVEÂ to point at the location of your top directory there, if you have one.
The table below summarized the available space and policiies for each location:
locationquotaexpiration timebacked upon login nodeson compute nodes
$HOME
100 GB per user
none
yes
yes
read-only
$SCRATCH
25 TB per user(1)
TBD*
no
yes
yes
$PROJECT
determined by RAC allocation
1 TB per default group(2)
none
yes
yes
read-only
$ARCHIVE
determined by RAC allocation(2)
none
dual-copy
no
no
(1)The SCRATCH policies are still subject to revision.
(2)There is no RAC mechanism to increase project ($PROJECT) and nearline ($ARCHIVE) quotas on Trillium.
Software
Trillium uses theÂ environment modulesÂ system to manage compilers, libraries, and other software packages. Modules dynamically modify your environment (e.g.,Â PATH,Â LD_LIBRARY_PATH) so you can access different versions of software without conflicts.
Commonly used module commands:
module load <module-name>Â â€“ Load the default version of a software package.
module load <module-name>/<module-version>Â â€“ Load a specific version.
module purgeÂ â€“ Unload all currently loaded modules.
module availÂ â€“ List available modules that can be loaded.
module listÂ â€“ Show currently loaded modules.
module spiderÂ orÂ module spider <module-name>Â â€“ Search for modules and their versions.
Handy abbreviations are available:
mlÂ â€“ Equivalent toÂ module list.
ml <module-name>Â â€“ Equivalent toÂ module load <module-name>.
When you have just logged in, only theÂ CCconfig,Â gentoo/2023Â andÂ miiÂ modules are loaded, which provide basic OS-level functionality. To get a standard set of compilers and libraries like on the other compute clusters in the Alliance, you load theÂ StdEnv/2023.
Tips for loading software
Properly managing your software environment is key to avoiding conflicts and ensuring reproducibility. Here are some best practices:
Avoid loading modules in yourÂ .bashrcÂ file. Doing so can cause unexpected behavior, particularly in non-interactive environments like batch jobs or remote shells.
Instead, load modules manually, from a separate script, or using module collections. This approach gives you more control and helps keep environments clean.
Load required modules inside your job script. This ensures that your job runs with the expected software environment, regardless of your interactive shell settings.
Be explicit about module versions. Short names likeÂ gccÂ will load the system default (e.g.,Â gcc/12.3), which may change in the future. Specify full versions (e.g.,Â gcc/13.3) for long-term reproducibility.
Resolve dependencies withÂ module spider. Some modules depend on others. UseÂ module spider <module-name>Â to discover which modules are required and how to load them in the correct order. For more, seeÂ Sub-command spider.
Using commercial software
You may be able to use commercial software on Trillium, but there are a few important considerations:
Bring your own license. You can use commercial software on Trillium if you have a valid license. If the software requires a license server, you can connect to it securely usingÂ SSH tunnelling.
We do not provide user-specific licenses. Due to the large and diverse user base, we cannot provide licenses for individual or specialized commercial packages.
Some widely useful commercial tools are available system-wide, such as compilers (Intel), math libraries (MKL), debuggers (DDT).
We're here to help. If you have a valid license and need help installing commercial software, feel free to contact us, we'll assist where possible.
Testing and debugging
Before submitting your job to the cluster, it's important to test your code to ensure correctness and determine the resources it requires.
Lightweight testsÂ can be run directly on the login nodes. As a rule of thumb, these should:
Run in under a few minutes
Use no more than 1â€“2 GB of memory
Use only 1â€“4 CPU cores
Use at most 1 GPU
You can also run the parallelÂ ARM DDTÂ debugger on the login nodes after loading it withÂ module load ddt-cpuÂ orÂ module load ddt-gpu
For tests that exceed login node limits or require dedicated resources, request an interactive debug job using theÂ debugjobÂ command on a login node:
$ debugjob

When run from a CPU login node, this command gives you an interactive shell on a CPU compute session for 1-hour. When running the debugjob command from the GPU login node, you get an interactive session with 1 GPU on a (shared) GPU compute node for two hours. A few variations of this command that you can use to request more resources for an interactive session, are given in the next table. Note that the more resources you request, the shorter the allowed walltime is (this helps makes sure that interactive session almost always start right away).
CommandSubclusterNumber of nodesNumber of CPU coresNumber of GPUsMemoryWalltime limit
debugjob
CPU
1
192
0
755GiB
60 minutes
debugjob 2
CPU
2
384
0
2x755GiB
30 minutes
debugjob
debugjob -g 1
GPU
1/4
24
1
188GiB
120 minutes
debugjob 1
debugjob -g 4
GPU
1
96
4
755GiB
30 minutes
debugjob 2
debugjob -g 8
GPU
2
192
8
2x755GiB
15 minutes
The shell environment in a debugjob will be similar to the environment you get when you have just logged in: only standard modules loaded, no internet access, no write access to the home and project file systems, and no job submissions. By the way, if you want the session to inherit the modules that you had loaded before issuing the debugjob command, you can add "--export=ALL" as the first option to debugjob.
If your test job requires more time than allowed byÂ debugjob, you can request an interactive session from the regular queue usingÂ salloc. For CPU test jobs, the command would be as follows:
$ salloc --export=NONE --nodes=N --time=M:00:00 [--ngpus-per-node=G] [--x11]

where
NÂ is the number of nodes
MÂ is the number of hours the job should run
GÂ is the number of GPUs per node (when applicable).
--x11Â is required for graphical applications (e.g., when usingÂ ARM DDT), but otherwise optional.
Note:Â Jobs submitted withÂ sallocÂ may take longer to start than with debugjob and count towards your allocation.
Submitting jobs to the scheduler
Once you have compiled and tested your code or workflow on the Trillium login nodes and confirmed that it behaves correctly, you are ready to submit jobs to the cluster. These jobs will run on Trillium's compute nodes, and their execution is managed by the scheduler.
Trillium uses SLURM as its job scheduler. More advanced details of how to interact with the scheduler can be found on theÂ Slurm page.
To submit a job, use theÂ sbatchÂ command on a login node:
$ sbatch jobscript.sh

CPU compute jobs need to be submitted from the CPU login nodes, while GPU compute nodes must be submitted from the GPU login node. In both cases, the command is the same, but the options inside the jobscript will have to be different (see below).
The sbatch command places your job into the queue. The job script should contain lines starting withÂ #SBATCHÂ that specify the resources that this script will need (the most common options will be given below). SLURM will begin execution of this script on compute nodes when your job is at the top of the priority queue and these resources are available.
The priority of a job in the queue depends on requested resources, time spent in the queue, recent past usage, as well as on the SLURM account under which the job was submitted. SLURM accounts correspond toÂ Resource Allocation Projects, or RAPs:
Each PI has at least one RAP, the RAS or default RAP. Users sponsored by that PI have access to the corresponding SLURM account, whose name starts withÂ def-.
PIs that have a RAC allocation have an additional RAC RAP, to which they can add users. The names of corresponding SLURM accounts typically start withÂ rrg-Â orÂ rpp-. Note that RACs are bound to a system, e.g. a RAC for Nibi cannot be used on Trillium.
Trillium specific restrictions
Because Trillium is designed as a system for large parallel jobs, there are some differences with the General Purpose clustersÂ Fir,Â Nibi,Â Narval, andÂ Rorqual, which we will now discuss.
Job output must be written to the scratch file system
The scratch file system is a fast parallel file system that you should use for writing out data during jobs. This is enforced by having the home and project directories only available for reading on the compute nodes.
In addition to making sure your application writes to scratch, in most cases, you should also submit your jobs from yourÂ $SCRATCHÂ directory (i.e. notÂ $HOMEÂ orÂ $PROJECT). The default location for the output files of SLURM are in the directory from which you submit, so if that is not in scratch, the output files would not be written.
Default scheduler account
Jobs will run under your group's RAC allocation, or if one is not available, under a RAS allocation. You can control this explicitly by specifying the account with theÂ --account=ACCOUNT_NAMEÂ option in your job script or submission command. For users with multiple allocations, specifying the account name is highly recommended.
No job submission from jobs
Jobs cannot be submitted from compute nodes (nor datamover nodes). This prevents accidentally spawning many jobs, overloading the scheduler, and overloading the backup process.
Whole node or whole gpu scheduling
It is not possible to request a certain number of core on Trillium. On the CPU subcluster, all jobs must use full nodes. That means the minimum size of a CPU job has 192 cores are its disposal which you must use effectively. If you are running serial or low-core-count jobs you must still use all 192 cores on the node by bundling multiple independent tasks in one job script. For examples, seeÂ GNU ParallelÂ andÂ this section of the META-Farm advanced page.
If your job underutilizes the cores, our support team may reach out to assist you in optimizing your workflow, or you canÂ contact usÂ to get assistance.
On the GPU subcluster, each node contains 4 GPUs. The scheduler allows you to request either a whole number of nodes, or a single GPU. The latter amounts to a quarter node, with 24 cores and about 188GiB of RAM. It is important to use the GPU efficiently. Trillium does not support MIG as on the other clusters (MIG allows you to schedule a fraction of a GPU), but you can useÂ Hyper-Q / MPSÂ within your jobs.
Memory requests are ignored
Memory requests are ignored. Your CPU jobs always receiveÂ N Ã— 768GBÂ of RAM, whereÂ NÂ is the number of nodes and 768GB is the amount of memory on each node. Your GPU full-node jobs get the same amount of memory, while single-GPU jobs get 1/4 of the memory, i.e., 188GiB.
Common options for job script
The following options are commonly used:
optionshort optionmeaningnotes
--nodes
-N
number of nodes
Recommended to always include this
--ntasks-per-node
number of tasks for srun/mpirun to launch per node
Prefer this overÂ --ntasks
--ntasks
-n
number of tasks for srun/mpirun to launch
--cpus-per-task
-c
number of cores per task;
Typically for (OpenMP) threads
--time
-t
duration of the job
--job-name
-J
specify a name for the job
--output
-o
file to redirect standard ouput to
Can be a pattern using e.g. %j for the jobid.
--mail-type
when to send email (e.g. BEGIN, END, FAIL, ALL)
--gpus-per-node
number of gpus to use on each node
Either 1 or 4 is allowed on the GPU subcluster
--partition
-p
partition to submit to
See below for available partitions
--account
-A
slurm account to use
For many users, this is automatic on Trillium
--mem
amount of memory requested
Ignored on Trillium, you get all the memory
These options should be put in separate comment lines at the top of the job script (but afterÂ #!/bin/bash), prefixed withÂ #SBATCH. They can also be used as command line options forÂ salloc. Some examples of job scripts are given below.
More options and details can be found on theÂ Running jobsÂ page and in theÂ SLURM documentation.
Submitting jobs on the CPU subcluster
Partitions and limits
There are limits to the size and duration of your jobs, the number of jobs you can run, and the number of jobs you can have queued. It matters whether a user is part of a group with a RAC allocation (e.g. an RRG or RPP) or not. It also matters in which "partition" the job runs. "Partitions" are SLURM-speak for use cases. You specify the partition with theÂ -pÂ parameter toÂ sbatchÂ orÂ salloc, but if you do not specify one, your job will run in theÂ computeÂ partition, which is the most common case.
UsagePartitionLimit on Running jobsLimit on Submitted jobs (incl. running)Min. size of jobsMax. size of jobsMin. walltimeMax. walltime
Compute jobs
compute
150
500
1 node (192Â cores)
default:Â 10Â nodesÂ (1920Â cores)
withÂ allocation:Â 128Â nodesÂ (24576Â cores)*
15 minutes
24 hours
Testing or troubleshooting
debug
1
1
1 node (192Â cores)
2 nodes (384 cores)
N/A
1 hour
*Â This is a safe-guard, if your rrg involves running larger jobs, let us know.
Even if you respect these limits, your jobs will still have to wait in the queue. The waiting time depends on many factors such as your group's allocation amount, how much allocation has been used in the recent past, the number of requested nodes and walltime, and how many other jobs are waiting in the queue.
Example: MPI job
#!/bin/bash
#SBATCH --nodes=2
#SBATCH --ntasks-per-node=192
#SBATCH --time=01:00:00
#SBATCH --job-name=mpi_job
#SBATCH --output=mpi_output_%j.txt
#SBATCH --mail-type=FAIL

cd $SLURM_SUBMIT_DIR

module load StdEnv/2023
module load gcc/12.3
module load openmpi/4.1.5

source /scinet/vast/etc/vastpreload-openmpi.bash # important if doing MPI-IO

mpirun ./mpi_example

Submit this script from a CPU login node while in yourÂ $SCRATCHÂ directory with the command:
$ sbatch mpi_job.sh

First line indicates that this is a bash script.
Lines starting withÂ #SBATCHÂ go to SLURM.
sbatchÂ reads these lines as a job request (which it gives the nameÂ mpi_job).
In this case, SLURM looks for 2 nodes each running 192 tasks, for 1 hour.
Once it finds such nodes, it runs the script, which does the following:
Change to the submission directory;
Loads modules;
Preloads a library tuning MPI-IO for the VAST file system; change this to source /scinet/vast/etc/vastpreload-intelmpi.bash if using IntelMPI instead of OpenMPI.Â Note:Â mpirunÂ must be used for the VAST preload library to take effect, it does not work withÂ srun.
Runs theÂ mpi_exampleÂ application (SLURM will informÂ mpirunÂ orÂ srunÂ how many processes to run).
Example: OpenMP job
#!/bin/bash
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=192
#SBATCH --time=01:00:00
#SBATCH --job-name=openmp_job
#SBATCH --output=openmp_output_%j.txt
#SBATCH --mail-type=FAIL

cd $SLURM_SUBMIT_DIR

module load StdEnv/2023
module load gcc/12.3

export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK

./openmp_example
# or "srun ./openmp_example"

Submit this script from a CPU login node while in yourÂ $SCRATCHÂ directory with the command:
$ sbatch openmp_job.sh

First line indicates that this is a Bash script.
Lines starting withÂ #SBATCHÂ are directives for SLURM.
sbatchÂ reads these lines as a job request (which it gives the nameÂ openmp_job).
In this case, SLURM looks for one node with 192 CPUs for a single task running up to 192 OpenMP threads, for 1 hour.
Once such a node is allocated, it runs the script:
Changes to the submission directory;
Loads the required modules;
SetsÂ OMP_NUM_THREADSÂ based on SLURMâ€™s CPU allocation;
Runs theÂ openmp_exampleÂ application.
Example: hybrid MPI/OpenMP job
#!/bin/bash
#SBATCH --nodes=2
#SBATCH --ntasks-per-node=48
#SBATCH --cpus-per-task=4
#SBATCH --time=01:00:00
#SBATCH --job-name=hybrid_job
#SBATCH --output=hybrid_output_%j.txt
#SBATCH --mail-type=FAIL

cd $SLURM_SUBMIT_DIR

module load StdEnv/2023
module load gcc/12.3
module load openmpi/4.1.5

export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK
export OMP_PLACES=cores
export OMP_PROC_BIND=true

export CORES_PER_L3CACHE=8
export RANKS_PER_L3CACHE=$(( $CORES_PER_L3CACHE / $OMP_NUM_THREADS ))  # this works up to 8 threads 

source /scinet/vast/etc/vastpreload-openmpi.bash # important if doing MPI-IO

mpirun --bind-to core --map-by ppr:$RANKS_PER_L3CACHE:l3cache:pe=$OMP_NUM_THREADS ./hybrid_example

Submit this script from a CPU login node while in yourÂ $SCRATCHÂ directory with the command:
$ sbatch hybrid_job.sh

First line indicates that this is a bash script.
Lines starting withÂ #SBATCHÂ go to SLURM.
sbatchÂ reads these lines as a job request (which it gives the nameÂ hybrid_job).
In this case, SLURM looks for 2 nodes each running 48 tasks, each with 4 threads for 1 hour.
Once it finds such a node, it runs the script:
Change to the submission directory;
Loads modules;
Preloads a library tuning MPI-IO for the VAST file system; change this to source /scinet/vast/etc/vastpreload-intelmpi.bash if using IntelMPI instead of OpenMPI.Â Note:Â mpirunÂ must be used for the VAST preload library to take effect, it does not work withÂ srun.
Runs theÂ hybrid_exampleÂ application. While SLURM will informÂ mpirunÂ how many processes to run, it needs help to spread the processes and threads evenly over the cores. The --map-by option solves this.
(for more than 8 and at most 24 threads per process, change 'l3cache' to 'numa' and for more than 24, change it to 'socket').
Submitting jobs for the GPU subcluster
Partitions and limits
As with the CPU subcluster, there are limits to the size and duration of your jobs, the number of jobs you can run, and the number of jobs you can have queued, and whether a user is part of a group with a RAC allocation or not. There are more partitions for this subcluster than for the CPU subcluster to support scheduling by GPU instead of by node (each node has 4 GPUs).
On Trillium, you are only allowed to request exactly 1 GPU or a multiple of 4 GPUs. You cannot request --gpus-per-node=2 or 3, nor can you use NVIDIA's MIG technology to allocate a subdivision of a GPU. Inside a job, you can use NVIDIA's Multi-Process Service (MPS) to share a GPU among processes running on the same job.
For single-GPU jobs, useÂ --gpus-per-node=1.
For whole-node GPU job, useÂ --gpus-per-node=4.
UsagePartitionLimit on Running jobsLimit on Submitted jobs (incl. running)Min. size of jobsMax. size of jobsMin. walltimeMax. walltime
GPU compute jobs
compute
150
500
1/4 node (24Â cores / 1GPU)
default:Â 5Â nodesÂ (480Â cores/20 GPUs)
withÂ allocation:Â 25Â nodesÂ (2400Â cores/100 GPUs)
15 minutes
24 hours
Testing GPU jobs
debug
1
1
1/4 node (24 cores / 1 GPU)
2 nodes (192 cores/ 8 GPUs)
N/A
2 hours (1 GPU) - 30 minutes (8 GPUs)
Even if you respect these limits, your jobs will still have to wait in the queue. The waiting time depends on many factors such as your group's allocation amount, how much allocation has been used in the recent past, the number of requested nodes and walltime, and how many other jobs are waiting in the queue.
Example: Single-GPU Job
#!/bin/bash
#SBATCH --job-name=single_gpu_job         # Job name
#SBATCH --output=single_gpu_job_%j.out    # Output file (%j = job ID)
#SBATCH --nodes=1                         # Request 1 node
#SBATCH --gpus-per-node=1                 # Request 1 GPU
#SBATCH --time=00:30:00                   # Max runtime (30 minutes)

# Load modules
module load StdEnv/2023
module load cuda/12.6
module load python/3.11.5

# Activate Python environment (if applicable)
source ~/myenv/bin/activate

# Check GPU allocation
srun nvidia-smi

# Run your workload
srun python my_script.py

Example: Whole-Node (4 GPUs) Job
#!/bin/bash
#SBATCH --job-name=whole_node_gpu_job
#SBATCH --output=whole_node_gpu_job_%j.out
#SBATCH --nodes=1
#SBATCH --gpus-per-node=4
#SBATCH --time=02:00:00

module load StdEnv/2023
module load cuda/12.6
module load python/3.11.5

# Activate Python environment (if applicable)
source ~/myenv/bin/activate

srun python my_distributed_script.py

Example: Multi-Node GPU Job
#!/bin/bash
#SBATCH --job-name=multi_node_gpu_job
#SBATCH --output=multi_node_gpu_job_%j.out
#SBATCH --nodes=2                        # Request 2 full nodes
#SBATCH --gpus-per-node=4                # 4 GPUs per node (full node)
#SBATCH --time=04:00:00

module load StdEnv/2023
module load cuda/12.6
module load openmpi/4.1.5

# Check all GPUs allocated
srun nvidia-smi

# Activate Python environment (if applicable)
source ~/myenv/bin/activate

# Example: run a distributed training job with 8 GPUs (2 nodes Ã— 4 GPUs)
srun python train_distributed.py

Best Practices for GPU Jobs
Do not useÂ --memÂ â€” memory is fixed per GPU (192 GB) or per node (768 GB).
Always specify node count, andÂ --gpus-per-node=4Â for whole-node or multi-node jobs.
Load only the modules you need â€” seeÂ Using modules.
Be explicit with software versions for reproducibility (e.g.,Â cuda/12.6Â rather than justÂ cuda).
Test on a single GPU before scaling to multiple GPUs or nodes.
Monitor usage withÂ nvidia-smiÂ to ensure GPUs are fully utilized.
Monitoring
Monitoring the queue
Once your job is submitted to the queue, you can monitor its status and performance using the following SLURM commands:
squeueÂ shows all jobs in the queue. UseÂ squeue -u $USERÂ to view only your jobs.
squeue -j JOBIDÂ shows the current status of a specific job. Alternatively, useÂ scontrol show job JOBIDÂ for detailed information, including allocated nodes, resources, and job flags.
squeue --start -j JOBIDÂ gives a rough estimate of when a pending job is expected to start. Note that this estimate is often inaccurate and can change depending on system load and priorities.
scancel JOBIDÂ cancels a job you submitted.
jobperf JOBIDÂ gives a live snapshot of the CPU and memory usage of your job while it is running.
sacctÂ shows information about your past jobs, including start time, run time, node usage, and exit status.
More details on monitoring jobs can be found on theÂ Slurm page.
Monitoring running and past jobs
Note that after your job has finished, it will be removed from the queue, so SLURM commands that query the queue like squeue and sacct will not find your job anymore.
Your past jobs and their resource usage can be inspected through theÂ my.SciNetÂ portal. This portal saves information about all jobs, including performance data collected every two minutes while the job was running.
Quick Reference for Common Commands
CommandDescription
sbatch <script>
Submit a batch job script
squeue [-u $USER]
View queued jobs (optionally for current user)
scancel <JOBID>
Cancel a job
sacct
View accounting data for recent past jobs
module load <module>
Load a software module
module list
List loaded modules
module avail
List available modules
module spider <module>
Search for modules and dependencies
debugjob [N]
Request a short debug job (on N nodes)
diskusage_report
Check storage quotas
jobperf <JOBID>
Monitor CPU and memory usage of a running job
nvidia-smi
Check GPU status (on GPU nodes)
Category:Â 
Pages using deprecated source tags
Navigation menu
English
Log in
Page
Discussion
Read
View source
View history
Search
Wiki Main Page
Support
Getting an account
Getting started
Getting help
Running jobs
Known issues
System status
Resources
Fir
Narval
Nibi
Rorqual
Trillium
Cloud
Killarney
tamIA
Vulcan
Quantum computing
Available software
The Alliance
Alliance website
CCDB
Resource Allocation Competition (RAC)
Acknowledging the Alliance
Alliance policies
Legacy resources
BÃ©luga
Cedar
Graham
Niagara
Authoring
Guidelines
MediaWiki Help
Recent changes
Tools
What links here
Related changes
Special pages
Printable version
Permanent link
Page information
This page was last edited on 4 November 2025, at 19:42.
Privacy policy
About Alliance Doc
Disclaimers
Mobile view
